 <!DOCTYPE html>
<html lang="en">
<head>
  <title>KEP-SVGP</title>
  <meta name="description" content="Project page for Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Facebook preview-->
  <meta property="og:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400">
  <meta property="og:type" content="website"/>
  <meta property="og:url" content="https://imagine.enpc.fr/~monniert/DTIClustering/"/>
  <meta property="og:title" content="DTI Clustering"/>
  <meta property="og:description" content="Project page for Deep Transformation-Invariant Clustering."/>

  <!--Twitter preview-->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="DTI Clustering" />
  <meta name="twitter:description" content="Project page for Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation."/>
  <meta name="twitter:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail_twitter.png">

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1 style="font-size:31.7px">Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes</h1>
    <h4 style="font-size:23.5px"><font color=#f26522>ICML 2024</font></h4>
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-2"></div>
    <div class="col-xs-12 col-md-8">
      <h4>
        <a href="https://yingyichen-cyy.github.io/"><nobr>Yingyi Chen</nobr></a><sup>* 1</sup> &emsp;
        <a href="https://qinghua-tao.github.io/"><nobr>Qinghua Tao</nobr></a><sup>* 1</sup> &emsp;
        <a href="https://taralloc.github.io/"><nobr>Francesco Tonin</nobr></a><sup>2</sup> &emsp;
        <a href="https://www.esat.kuleuven.be/sista/members/suykens.html"><nobr>Johan A.K. Suykens</nobr></a><sup>1</sup> 
      </h4>
      <h4 style="font-size:17px"><sup>1</sup>ESAT-STADIUS, KU Leuven, Belgium &nbsp&nbsp&nbsp&nbsp&nbsp</a><sup>2</sup>LIONS, EPFL, Switzerland</a></h4> 
      <h4 style="font-size:17px"><sup>*</sup>Equal contribution&emsp;</h4>
    </div>
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="resrc/workflow.jpg" alt="workflow.jpg" class="text-center" style="width: 95%; max-width: 600px">
  <br/>
  <br/>
  <p><b>Illustration of canonical self-attention and our KEP-SVGP in one layer.</b> 
  (a) The attention kernel 
  <img src="http://latex.codecogs.com/svg.latex?K_{\rm att}" alt="K_{\rm att}" border="0"/>
  in canonical self-attention is induced by two different feature maps 
  <img src="http://latex.codecogs.com/svg.latex?{\phi_q,\phi_k}" alt="\phi_q,\phi_k" border="0"/>
  related to queries and keys; hence 
  <img src="http://latex.codecogs.com/svg.latex?K_{\rm att}" alt="K_{\rm att}" border="0"/>
  is in essence asymmetric.
  (b) KEP-SVGP consists of one SVGP pair induced by the two sets of projection outputs based on 
  <img src="http://latex.codecogs.com/svg.latex?{\phi_q,\phi_k}" alt="\phi_q,\phi_k" border="0"/>
  from KSVD w.r.t. 
  <img src="http://latex.codecogs.com/svg.latex?K_{\rm att}" alt="K_{\rm att}" border="0"/>, 
  which fully characterizes the asymmetry of self-attention in the posterior. 
  The posteriors are now approximated based on the inversion of a diagonal matrix 
  <img src="http://latex.codecogs.com/svg.latex?\Lambda" alt="\Lambda" border="0"/>
  containing top 
  <img src="http://latex.codecogs.com/svg.latex?s" alt="s" border="0"/> 
  singular values, thereby of time complexity 
  <img src="http://latex.codecogs.com/svg.latex?\mathcal{O}(s)" alt="\mathcal{O}(s)" border="0"/>.
  </p>
  <h3 style="text-align:center; padding-top:1rem">
    <!-- <a class="label label-info" href="">arXiv</a> -->
    <a class="label label-info" href="https://arxiv.org/abs/2402.01476">arXiv</a>
    <a class="label label-info" href="https://openreview.net/forum?id=4RqG4K5UwL">Paper</a>
    <a class="label label-info" href="https://github.com/yingyichen-cyy/KEP-SVGP">Code</a>
    <a class="label label-info" href="">Video</a>
    <a class="label label-info" href="">Poster</a>
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
    While the great capability of Transformers significantly boosts prediction accuracy, it could also yield overconfident predictions and require calibrated uncertainty estimation, which can be commonly tackled by Gaussian processes (GPs).
    Existing works apply GPs with symmetric kernels under variational inference to the attention kernel; however, omitting the fact that attention kernels are in essence asymmetric.
    Moreover, the complexity of deriving the GP posteriors remains high for large-scale data.
    In this work, we propose Kernel-Eigen Pair Sparse Variational Gaussian Processes (KEP-SVGP) for building uncertainty-aware self-attention where the asymmetry of attention kernels is tackled by Kernel SVD (KSVD) and a reduced complexity is acquired.
    Through KEP-SVGP, 
    <i>i)</i> the SVGP pair induced by the two sets of singular vectors from KSVD w.r.t. the attention kernel fully characterizes the asymmetry;
    <i>ii)</i> using only a small set of adjoint eigenfunctions from KSVD, the derivation of SVGP posteriors can be based on the inversion of a diagonal matrix containing singular values, contributing to a reduction in time complexity;
    <i>iii)</i> an evidence lower bound is derived so that variational parameters and network weights can be optimized with it.
    Experiments verify our excellent performances and efficiency on in-distribution, distribution-shift and out-of-distribution benchmarks.
  </p>

  <h3>Background</h3>
  <hr/>
  <p><h4 style="text-align: center"><u>Gaussian Processes</u></h4></p>
  A Gaussian Process (GP) represents a distribution, denoted by <img src="http://latex.codecogs.com/svg.latex?\mathcal{GP}" alt="" border="0"/>, over real-valued functions <img src="http://latex.codecogs.com/svg.latex?f(\cdot):\mathcal{X}\to\mathbb{R}" alt="" border="0"/> defined on an input domain <img src="http://latex.codecogs.com/svg.latex?\mathcal{X}\subset\mathbb{R}^d" alt="" border="0"/>. We have the prior process as follows:
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?
    \begin{align*}
    & \text{Prior:}\quad f(\cdot)\sim\mathcal{GP}
    (0,\kappa(\cdot,\cdot))
    \Rightarrow
    \boldsymbol{f}\sim\mathcal{N}(\boldsymbol{0}, K_{XX}),\quad
    K_{XX}:=[\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)]\in\mathbb{R}^{N\times N}
    \\
    & \text{Posterior:}\quad
    \boldsymbol{f}^*|X^*,X,\boldsymbol{y}
    \sim \mathcal{N}\left(K_{X^*X}{(K_{XX}+\sigma^2I_N)^{-1}}\boldsymbol{y}, 
    K_{X^*X^*}-K_{X^*X}{\color{red}(K_{XX}+\sigma^2I_N)^{-1}}K_{XX^*}\right),
    \end{align*}" border="0"/>
  </center>
  <p></p>
  where <img src="http://latex.codecogs.com/svg.latex?\boldsymbol{f}:=[f(\boldsymbol{x}_1),\ldots, f(\boldsymbol{x}_N)]^\top\in\mathbb{R}^N" alt="" border="0"/> and the GP is with a symmetric positive-definite covariance function parameterized by a kernel function
  <img src="http://latex.codecogs.com/svg.latex?\kappa(\cdot,\cdot):\mathcal{X}\times\mathcal{X}\to\mathbb{R}" alt="" border="0"/>.
  The posterior process of <img src="http://latex.codecogs.com/svg.latex?\boldsymbol{f}^*" alt="" border="0"/> is
  with time complexity <img src="http://latex.codecogs.com/svg.latex?{\color{red}\mathcal{O}(N^3)}" alt="" border="0"/>, due to the inversion of the kernel matrix.

  <p><h4 style="text-align: center"><u>Sparse Variational Gaussian Processes</u></h4></p>
  Sparse variational Gaussian processes (SVGP) use <img src="http://latex.codecogs.com/svg.latex?M" alt="" border="0"/> inducing points <img src="http://latex.codecogs.com/svg.latex?\{Z_1,\ldots,Z_M\}\in\mathcal{X}" alt="" border="0"/> to represent the whole dataset, <img src="http://latex.codecogs.com/svg.latex?u[m]=f(Z_m)" alt="" border="0"/>
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?
    \begin{align*}
    & \text{Prior:}\quad 
    \begin{pmatrix}
      f(\cdot) \\ \boldsymbol{u}
    \end{pmatrix}
    \sim
    \mathcal{GP}
    \left( \boldsymbol{0},
    \begin{bmatrix}
        \kappa(\cdot,\cdot') & \boldsymbol{\kappa}_{\cdot\boldsymbol{u}}
        \\
        \boldsymbol{\kappa}_{\boldsymbol{u}\cdot'} & K_{\boldsymbol{uu}}
    \end{bmatrix}
    \right)
    \\
    & \text{Posterior:}\quad
    f(\cdot)
    \sim \mathcal{GP}\left(\boldsymbol{\kappa}_{\cdot \boldsymbol{u}}
    K_{\boldsymbol{uu}^{-1}}\boldsymbol{m_u},
    \,\,
    \kappa(\cdot,\cdot') - \boldsymbol{\kappa}_{\cdot\boldsymbol{u}}
    {\color{red}K_{\boldsymbol{uu}}^{-1}}
    (K_{\boldsymbol{uu}}-S_{\boldsymbol{uu}})
    K_{\boldsymbol{uu}}^{-1}\boldsymbol{\kappa}_{\boldsymbol{u}\cdot'}
    \right),
    \end{align*}
    " border="0"/>
  </center>
  <p></p>
  where the variational posterior is based on <img src="http://latex.codecogs.com/svg.latex?\mathrm{q}(f(\cdot))=\int \mathrm{p}(f(\cdot)|\boldsymbol{u})\mathrm{q}(\boldsymbol{u})\,\mathrm{d}\boldsymbol{u}" alt="" border="0"/> with <img src="http://latex.codecogs.com/svg.latex?\mathrm{q}(\boldsymbol{u})=\mathcal{N}(\boldsymbol{m_u},S_{\boldsymbol{uu}})" alt="" border="0"/> being the variational distribuition of the inducing points. The posteior is now with time complexity <img src="http://latex.codecogs.com/svg.latex?{\color{red}\mathcal{O}(M^3),\, M<N}" alt="" border="0"/>.

  <p><h4 style="text-align: center"><u>SVGPs with Kernel-Eigen Features</u></h4></p>
  We can further reduce the time compleixty of the posterior process by choosing kernel-eigen features as the inducing variables. 
  Define <img src="http://latex.codecogs.com/svg.latex?\boldsymbol{u}_i:=\nu_i(\cdot)" alt="" border="0"/> as the <img src="http://latex.codecogs.com/svg.latex?i" alt="" border="0"/>-th eigenfunction of the kernel <img src="http://latex.codecogs.com/svg.latex?\kappa(\cdot,\cdot)" alt="" border="0"/> with eigenvalue <img src="http://latex.codecogs.com/svg.latex?\lambda_i" alt="" border="0"/>, then we have
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?
    \begin{align*}
    & \text{Prior:}\quad 
    \begin{pmatrix}
      \boldsymbol{f} \\ \boldsymbol{u}
    \end{pmatrix}
    \sim 
    \mathcal{GP}
    \left(
    \begin{bmatrix}
        K_{XX} & H\Lambda
        \\
        \Lambda H^\top & \Lambda
    \end{bmatrix}
    \right)
    \\
    & \text{Posterior:}\quad  
    \boldsymbol{f}
    \sim
    \mathcal{N}
    \left(
    (H\Lambda)\Lambda^{-1}\boldsymbol{m_u}
    \,\,
    K_{XX} - (H\Lambda){\color{red}\Lambda^{-1}}(\Lambda-S_{\boldsymbol{uu}})\Lambda^{-1}(\Lambda H^\top)
    \right)
    ,
    \end{align*}
    " border="0"/>
  </center>
  <p></p>
  where <img src="http://latex.codecogs.com/svg.latex?\Lambda:=\text{diag}\{\lambda_1,\ldots,\lambda_M\}" alt="" border="0"/> is the eigenvalue matrix, and <img src="http://latex.codecogs.com/svg.latex?H=[\nu_1,\ldots,\nu_M]\in\mathbb{R}^{N\times M}" alt="" border="0"/> are the eigenvectors.
  The posteior is now with time complexity <img src="http://latex.codecogs.com/svg.latex?{\color{red}\mathcal{O}(M),\, M<N}" alt="" border="0"/>.
 
  <p><h4 style="text-align: center"><u>Canonical Self-Attention is with Asymmetric Kernel</u></h4></p>
  Let <img src="http://latex.codecogs.com/svg.latex?\{\boldsymbol{x}_i\in\mathbb{R}^d\}_{i=1}^N" alt="\{\boldsymbol{x}_i\in\mathbb{R}^d\}_{i=1}^N" border="0"/> be the input data sequence.
  In self-attention, the queries, keys and values output the linear projections of the input sequence:
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?q(\boldsymbol{x}_i) = W_q \boldsymbol{x}_i,\quad k(\boldsymbol{x}_i) = W_k \boldsymbol{x}_i,\quad
    v(\boldsymbol{x}_i) = W_v \boldsymbol{x}_i." alt="q(\boldsymbol{x}_i) = W_q \boldsymbol{x}_i,
    \quad
    k(\boldsymbol{x}_i) = W_k \boldsymbol{x}_i,
    \quad
    v(\boldsymbol{x}_i) = W_v \boldsymbol{x}_i." border="0"/>
  </center>
  <p></p>
  The canonical self-attention is with "softmax" activation applied for bringing non-linearity and positives, yielding the attention weights:
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?\kappa_\text{att} (\boldsymbol{x}_i, \boldsymbol{x}_j) = \text{softmax} \left( \left< W_q \boldsymbol{x}_i, W_k \boldsymbol{x}_j\right> / \sqrt{d_k} \right), \quad i,j=1,\ldots,N," alt="\kappa_\text{att} (\boldsymbol{x}_i, \boldsymbol{x}_j)= \text{softmax} \left( \left< W_q \boldsymbol{x}_i, W_k \boldsymbol{x}_j\right> / \sqrt{d_k} \right), \quad i,j=1,\ldots,N," border="0"/>
  </center>
  <p></p>
  where <img src="http://latex.codecogs.com/svg.latex?\kappa_\text{att}(\cdot,\cdot):\mathbb{R}^d\times\mathbb{R}^d\mapsto \mathbb{R}" alt="\kappa_\text{att}(\cdot,\cdot):\mathbb{R}^d\times\mathbb{R}^d\mapsto \mathbb{R}" border="0"/>
  serves as the kernel function. 
  Notice that in general, 
  <img src="http://latex.codecogs.com/svg.latex?\left< W_q \mathbf{x}_i, W_k \boldsymbol{x}_j\right> \neq \left< W_q \boldsymbol{x}_j, W_k \mathbf{x}_i\right>" alt="\left< W_q \boldsymbol{x}_i, W_k \boldsymbol{x}_j\right> \neq \left< W_q \boldsymbol{x}_j, W_k \boldsymbol{x}_i\right>" border="0"/>,
  leading to <b>an asymmeyric kernel</b>
  where 
  <img src="http://latex.codecogs.com/svg.latex?\kappa_\text{att}(\boldsymbol{x}_i, \boldsymbol{x}_j)\neq \kappa_\text{att}(\boldsymbol{x}_j, \boldsymbol{x}_i)" alt="" border="0"/>.
  Please refer to <a href="https://openreview.net/forum?id=bRyduWAAVT">Primal-Attention (NeurIPS 2023)</a> for more details.
  
  </br></br>
  <p><h4 style="text-align: center"><u>Self-Attention with Kernel SVD</u></h4></p>
  Canonical self-attention can be represented by the dual representation of the kernel SVD (KSVD) problem. Please refer to <a href="https://openreview.net/forum?id=bRyduWAAVT">Primal-Attention (NeurIPS 2023)</a> for more details.

  </br></br>
  <p><b>Remark 3.3 (Primal-dual representations of KSVD in self-attention).</b> 
  <i>In the KSVD formulations for the asymmetric kernel matrix in self-attention, with KKT conditions, the projection scores can be either represented in the primal using explicit feature maps or in the dual using  kernel functions:
  </i>
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?
         \text{Primal:}\, 
         \begin{cases}
            e(\boldsymbol{x}) = W_{e|X}^\top \phi_q(\boldsymbol{x})
            \\
            r(\boldsymbol{x}) = W_{r|X}^\top \phi_k(\boldsymbol{x})
         \end{cases},
         \quad
         \text{Dual:}\, 
         \begin{cases}
            e(\boldsymbol{x}) = \sum\nolimits_{j=1}^N \boldsymbol{h}_{r_j} \kappa_\text{att}(\boldsymbol{x},\boldsymbol{x}_j) 
            \\
            r(\boldsymbol{x}) = \sum\nolimits_{i=1}^N \boldsymbol{h}_{e_i} \kappa_\text{att}(\boldsymbol{x}_i,\boldsymbol{x}) 
         \end{cases}.
         " alt="" border="0"/>
  </center>
  <p></p>
  where <img src="http://latex.codecogs.com/svg.latex?H_e:=[\boldsymbol{h}_{e_1},\ldots,\boldsymbol{h}_{e_N}]^\top" alt="" border="0"/>, <img src="http://latex.codecogs.com/svg.latex?H_r:=[\boldsymbol{h}_{r_1},\ldots,\boldsymbol{h}_{r_N}]^\top \in\mathbb{R}^{N\times s}" alt="" border="0"/> are dual variables with column-wisely the left and right singular vectors of the attention matrix <img src="http://latex.codecogs.com/svg.latex?K_{\text{att}}" alt="" border="0"/>.

  <h3>Method</h3>
  <hr/>

  <p><h4 style="text-align: center"><u>Pair of Adjoint Eigenfunctions for Self-Attention</u></h4></p>
  Self-Attention corresponds to a shifted eigenvalue problem w.r.t. the attention matrix [<a href="https://openreview.net/forum?id=bRyduWAAVT">Primal-Attention (NeurIPS 2023)</a>]:
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?
         \begin{align*}
         K_{\text{att}} H_r=H_e \Lambda, 
          \\
          K_{\text{att}}^\top H_e=H_r\Lambda,
         \end{align*}
         " alt="" border="0"/>
  </center>
  <p></p>
  leading to the two eigendecompositions w.r.t. symmetric kernel <img src="http://latex.codecogs.com/svg.latex?K_\text{att}K_\text{att}^\top" alt="" border="0"/>, <img src="http://latex.codecogs.com/svg.latex?K_\text{att}^\top K_\text{att}" alt="" border="0"/>:
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?
         \begin{align*}
         \left(K_{\text{att}}K_{\text{att}}^\top\right) H_e=H_e \Lambda^2, 
          \\
          \left(K_{\text{att}}^\top K_{\text{att}}\right) H_r=H_r\Lambda^2,
         \end{align*}
         " alt="" border="0"/>
  </center>
  <p></p>
  corresponding to two SVGPs with adjoint kernel-eigen features
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?
    \begin{align*}
    & \text{Prior:}\quad
    \begin{pmatrix}
      \boldsymbol{f}^e \\ \boldsymbol{u}^e
    \end{pmatrix}
    \sim 
    \mathcal{GP}
    \left(
    \begin{bmatrix}
        K_{\text{att}} K_{\text{att}}^\top & H_e\Lambda^2
        \\
        \Lambda^2 H_e^\top & \Lambda^2
    \end{bmatrix}
    \right)
    \quad
    \Rightarrow
    \quad
    \text{Posterior:}\quad
    \boldsymbol{f}^e
    \sim
    \mathcal{N}
    \Big(
    \underbrace{e(X)\Lambda^{-1}\boldsymbol{m_u}}_{\boldsymbol{m}^e}
    \,\,
    \underbrace{e(X)\Lambda^{-2}S_{\boldsymbol{uu}}e(X)^\top}_{\Sigma^e:=L^e {L^e}^\top}
    \Big),
    \\
    & \text{Prior:}\quad
    \begin{pmatrix}
      \boldsymbol{f}^r \\ \boldsymbol{u}^r
    \end{pmatrix}
    \sim 
    \mathcal{GP}
    \left(
    \begin{bmatrix}
        K_{\text{att}}^\top K_{\text{att}}  & H_r\Lambda^2
        \\
        \Lambda^2 H_r^\top & \Lambda^2
    \end{bmatrix}
    \right)
    \quad
    \Rightarrow
    \quad
    \text{Posterior:}
    \quad
    \boldsymbol{f}^r
    \sim
    \mathcal{N}
    \Big(
    \underbrace{r(X)\Lambda^{-1}\boldsymbol{m_u}}_{\boldsymbol{m}^r}
    \,\,
    \underbrace{r(X)\Lambda^{-2}S_{\boldsymbol{uu}}r(X)^\top}_{\Sigma^r:=L^r {L^r}^\top}
    \Big),
    \end{align*}
    " border="0"/>
  </center>
  <p></p>

  <p><h4 style="text-align: center"><u>Kernel-Eigen Pair Sparse Variational Gaussian Processes</u></h4></p>
  The outputs of the two SVGPs are obtained by the Monte-Carlo sampling:
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?
         \begin{align*}
         F^e=\boldsymbol{m}^e + L^e \boldsymbol{\epsilon},\quad \boldsymbol{\epsilon}\sim\mathcal{N}(0,\boldsymbol{I}_N),
          \\
         F^r=\boldsymbol{m}^r + L^r \boldsymbol{\epsilon},\quad \boldsymbol{\epsilon}\sim\mathcal{N}(0,\boldsymbol{I}_N),
         \end{align*}
         " alt="" border="0"/>
  </center>
  <p></p>
  where <img src="http://latex.codecogs.com/svg.latex?L^e,\,L^r" alt="" border="0"/> are the Cholesky factor of <img src="http://latex.codecogs.com/svg.latex?\Sigma^e,\,\Sigma^r" alt="" border="0"/> respectively.
  Finally, we merge the outputs of the two SVGPs either by addition or concatenation schemes:
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?
         \begin{align*}
         & \text{Addition:}\quad
          F^\text{add}:=F^e+F^r \in \mathbb{R}^N,
          \\
         & \text{Concatenation:}\quad
         F^\text{cat}:=[F^e; F^r]\in\mathbb{R}^{2N}.
         \end{align*}
         " alt="" border="0"/>
  </center>
  <p></p>
  The final outputs of the self-attention are
  <p></p>
  <center>
    <img src="http://latex.codecogs.com/svg.latex?
         \begin{align*}
         & \text{Addition:}\quad
          O:=F^\text{add} W^\text{add},
          \\
         & \text{Concatenation:}\quad
         O:= W^\text{cat}_1 F^\text{cat} W^\text{cat}_2.
         \end{align*}
         " alt="" border="0"/>
  </center>
  <p></p>

  <h3>Results</h3>
  <hr/>
  <p><font color=#27d13a>Please refer to our paper for more experiments.</font></p>
  <p><h4 style="text-align: center"><u>Uncertainty Awareness on In-distribution Data</u></h4></p>
  <div class="container" style="text-align:center; padding:1rem">
    <img src="resrc/in-distribution.png" alt="low-rank.jpg" class="text-center" style="width: 90%; max-width: 1100px">
    <br/>
  </div>


  <h3>Resources</h3>
  <hr/>
  <div class="row" style="text-align: center">
    <div class="col-xs-3 col-lg-3"></div>
    <div class="col-xs-2 col-lg-2">
      <h4>arXiv</h4>
      <a href="https://arxiv.org/abs/2402.01476" style="color:inherit"> 
        <img src="resrc/paper.jpg" alt="paper.jpg" class="text-center" style="max-width:100%; border:0.15em solid;
        border-radius:0.5em;"></a>
    </div>
    <div class="col-xs-2 col-lg-2">
      <h4>Paper</h4>
      <a href="https://openreview.net/forum?id=4RqG4K5UwL" style="color:inherit"> 
        <img src="resrc/openreview.jpg" alt="openreview.jpg" class="text-center" style="max-width:100%; border:0.15em solid;
        border-radius:0.5em;"></a>
    </div>
    <div class="col-xs-2 col-lg-2">
      <h4>Code</h4>
      <a href="https://github.com/yingyichen-cyy/KEP-SVGP" style="color:inherit;">
        <img src="resrc/github_repo.jpg" alt="github_repo.jpg" class="text-center"
             style="max-width:100%; border:0.15em solid;border-radius:0.5em;"></a>
    </div>
<!--     <div class="col-xs-2 col-lg-2">
      <h4>Poster</h4>
      <a href=resrc/poster.pdf style="color:inherit;">
        <img src="resrc/poster.png" alt="github_repo.jpg" class="text-center"
             style="max-width:100%; border:0.15em solid;border-radius:0.5em;"></a>
    </div>
    <div class="col-xs-2 col-lg-2">
      <h4>Video</h4>
      <a href="https://nips.cc/virtual/2023/poster/71144" style="color:inherit;">
        <img src="resrc/video.png" alt="video.png" class="text-center"
             style="max-width:100%; border:0.15em solid;border-radius:0.5em;"></a>
    </div> -->
  </div>

    <h4 style="padding-top:0.5em">BibTeX</h4>
    If you find this work useful for your research, please consider citing:
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
          @inproceedings{chen2024self,
            title={Self-Attention through Kernel-Eigen Pair Sparse Variational Gaussian Processes},
            author={Chen, Yingyi and Tao, Qinghua and Tonin, Francesco and Suykens, Johan A.K.},
            booktitle={International Conference on Machine Learning},
            year={2024}
          }</pre>
      </div>
    </div>

  <h3>Acknowledgements</h3>
  <hr/>
  <p>
    This work is jointly supported by the European Research Council under the European Union’s Horizon 2020 research and innovation program/ERC Advanced Grant E-DUALITY (787960), iBOF project Tensor Tools for Taming the Curse (3E221427), Research Council KU Leuven: Optimization framework for deep kernel machines C14/18/068,  KU Leuven Grant CoE PFV/10/002, The Research Foundation–Flanders (FWO) projects: GOA4917N (Deep Restricted kernel Machines: Methods and Foundations), Ph.D./Postdoctoral grant, the Flemish Government (AI Research Program), EU H2020 ICT-48 Network TAILOR (Foundations of Trustworthy AI-Integrating Reasoning, Learning and Optimization), Leuven.AI Institute.
  </p>
</div>

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  &#169; This webpage was in part inspired from this
  <a href="https://github.com/monniert/project-webpage">template</a>.
  </p>
</div>

</body>
</html> 
